# Getting Started / Primeiros Passos

*A beginner's guide to the awesome-data-science-toolkit / Um guia para iniciantes do awesome-data-science-toolkit*

## Installation / Instala√ß√£o

### English
To get started with this toolkit, you'll need to install the required dependencies:

```bash
# Install Python packages
pip install pandas numpy scikit-learn matplotlib seaborn plotly jupyter

# Optional: Install additional packages for advanced features
pip install xgboost lightgbm optuna
```

### Portugu√™s
Para come√ßar com este toolkit, voc√™ precisar√° instalar as depend√™ncias necess√°rias:

```bash
# Instalar pacotes Python
pip install pandas numpy scikit-learn matplotlib seaborn plotly jupyter

# Opcional: Instalar pacotes adicionais para funcionalidades avan√ßadas
pip install xgboost lightgbm optuna
```

## Basic Imports / Importa√ß√µes B√°sicas

```python
# Data manipulation / Manipula√ß√£o de dados
import pandas as pd
import numpy as np

# Machine Learning / Aprendizado de M√°quina
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report

# Data visualization / Visualiza√ß√£o de dados
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px

# Suppress warnings / Suprimir avisos
import warnings
warnings.filterwarnings('ignore')
```

## Data Loading and Cleaning / Carregamento e Limpeza de Dados

### English
Here's how to load and clean your dataset:

### Portugu√™s
Aqui est√° como carregar e limpar seu conjunto de dados:

```python
# Load data / Carregar dados
# Replace 'your_data.csv' with your actual file path
# Substitua 'your_data.csv' pelo caminho real do seu arquivo
df = pd.read_csv('your_data.csv')

# Basic data exploration / Explora√ß√£o b√°sica dos dados
print("Dataset shape / Formato do dataset:", df.shape)
print("\nFirst 5 rows / Primeiras 5 linhas:")
print(df.head())

print("\nData types / Tipos de dados:")
print(df.dtypes)

print("\nMissing values / Valores ausentes:")
print(df.isnull().sum())

# Handle missing values / Tratar valores ausentes
# Option 1: Remove rows with missing values / Op√ß√£o 1: Remover linhas com valores ausentes
df_clean = df.dropna()

# Option 2: Fill missing values / Op√ß√£o 2: Preencher valores ausentes
# For numerical columns / Para colunas num√©ricas
numerical_cols = df.select_dtypes(include=[np.number]).columns
df[numerical_cols] = df[numerical_cols].fillna(df[numerical_cols].mean())

# For categorical columns / Para colunas categ√≥ricas
categorical_cols = df.select_dtypes(include=['object']).columns
df[categorical_cols] = df[categorical_cols].fillna(df[categorical_cols].mode().iloc[0])

# Remove duplicates / Remover duplicatas
df = df.drop_duplicates()

print(f"\nCleaned dataset shape / Formato do dataset limpo: {df.shape}")
```

## Data Preparation / Prepara√ß√£o dos Dados

```python
# Encode categorical variables / Codificar vari√°veis categ√≥ricas
label_encoders = {}
for column in categorical_cols:
    if column in df.columns:  # Check if column still exists / Verificar se a coluna ainda existe
        le = LabelEncoder()
        df[column] = le.fit_transform(df[column].astype(str))
        label_encoders[column] = le

# Separate features and target / Separar caracter√≠sticas e vari√°vel alvo
# Assume the last column is the target / Assume que a √∫ltima coluna √© o alvo
X = df.iloc[:, :-1]  # Features / Caracter√≠sticas
y = df.iloc[:, -1]   # Target / Alvo

# Split data into training and testing sets / Dividir dados em conjuntos de treino e teste
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

# Scale the features / Escalar as caracter√≠sticas
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

print(f"Training set shape / Formato do conjunto de treino: {X_train_scaled.shape}")
print(f"Test set shape / Formato do conjunto de teste: {X_test_scaled.shape}")
```

## Basic Machine Learning / Aprendizado de M√°quina B√°sico

### English
Let's train a simple Random Forest model:

### Portugu√™s
Vamos treinar um modelo Random Forest simples:

```python
# Create and train the model / Criar e treinar o modelo
model = RandomForestClassifier(
    n_estimators=100,    # Number of trees / N√∫mero de √°rvores
    random_state=42,     # For reproducibility / Para reprodutibilidade
    max_depth=10,        # Maximum depth of trees / Profundidade m√°xima das √°rvores
    min_samples_split=5  # Minimum samples to split / Amostras m√≠nimas para dividir
)

# Train the model / Treinar o modelo
model.fit(X_train_scaled, y_train)

# Make predictions / Fazer previs√µes
y_pred = model.predict(X_test_scaled)

# Evaluate the model / Avaliar o modelo
accuracy = accuracy_score(y_test, y_pred)
print(f"Model Accuracy / Acur√°cia do Modelo: {accuracy:.4f}")

print("\nDetailed Classification Report / Relat√≥rio Detalhado de Classifica√ß√£o:")
print(classification_report(y_test, y_pred))

# Feature importance / Import√¢ncia das caracter√≠sticas
feature_importance = pd.DataFrame({
    'feature': X.columns,
    'importance': model.feature_importances_
}).sort_values('importance', ascending=False)

print("\nTop 10 Most Important Features / Top 10 Caracter√≠sticas Mais Importantes:")
print(feature_importance.head(10))
```

## Data Visualization / Visualiza√ß√£o de Dados

### English
Create informative visualizations to understand your data:

### Portugu√™s
Crie visualiza√ß√µes informativas para entender seus dados:

```python
# Set up the plotting style / Configurar o estilo dos gr√°ficos
plt.style.use('seaborn-v0_8')
sns.set_palette("husl")

# 1. Distribution of target variable / Distribui√ß√£o da vari√°vel alvo
plt.figure(figsize=(10, 6))
sns.countplot(data=df, x=df.columns[-1])
plt.title('Target Variable Distribution / Distribui√ß√£o da Vari√°vel Alvo')
plt.xlabel('Classes / Classes')
plt.ylabel('Count / Contagem')
plt.show()

# 2. Correlation heatmap / Mapa de calor de correla√ß√£o
plt.figure(figsize=(12, 8))
correlation_matrix = df.select_dtypes(include=[np.number]).corr()
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0, 
            square=True, linewidths=0.5)
plt.title('Feature Correlation Heatmap / Mapa de Calor de Correla√ß√£o das Caracter√≠sticas')
plt.tight_layout()
plt.show()

# 3. Feature importance visualization / Visualiza√ß√£o da import√¢ncia das caracter√≠sticas
plt.figure(figsize=(10, 8))
top_features = feature_importance.head(10)
sns.barplot(data=top_features, y='feature', x='importance', palette='viridis')
plt.title('Top 10 Feature Importance / Top 10 Import√¢ncia das Caracter√≠sticas')
plt.xlabel('Importance Score / Pontua√ß√£o de Import√¢ncia')
plt.ylabel('Features / Caracter√≠sticas')
plt.tight_layout()
plt.show()

# 4. Interactive plot with Plotly / Gr√°fico interativo com Plotly
fig = px.scatter_matrix(
    df.select_dtypes(include=[np.number]).sample(1000),  # Sample for performance
    title="Interactive Feature Scatter Matrix / Matriz de Dispers√£o Interativa das Caracter√≠sticas"
)
fig.show()

# 5. Model performance visualization / Visualiza√ß√£o do desempenho do modelo
from sklearn.metrics import confusion_matrix

plt.figure(figsize=(8, 6))
cm = confusion_matrix(y_test, y_pred)
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', 
            xticklabels=np.unique(y), yticklabels=np.unique(y))
plt.title('Confusion Matrix / Matriz de Confus√£o')
plt.xlabel('Predicted / Previsto')
plt.ylabel('Actual / Real')
plt.show()
```

## Advanced Tips / Dicas Avan√ßadas

### English
- **Cross-validation**: Use `cross_val_score` for more robust model evaluation
- **Hyperparameter tuning**: Try `GridSearchCV` or `RandomizedSearchCV`
- **Feature engineering**: Create new features based on domain knowledge
- **Ensemble methods**: Combine multiple models for better performance

### Portugu√™s
- **Valida√ß√£o cruzada**: Use `cross_val_score` para avalia√ß√£o mais robusta do modelo
- **Ajuste de hiperpar√¢metros**: Experimente `GridSearchCV` ou `RandomizedSearchCV`
- **Engenharia de caracter√≠sticas**: Crie novas caracter√≠sticas baseadas no conhecimento do dom√≠nio
- **M√©todos de ensemble**: Combine m√∫ltiplos modelos para melhor desempenho

```python
# Example of cross-validation / Exemplo de valida√ß√£o cruzada
from sklearn.model_selection import cross_val_score

cv_scores = cross_val_score(model, X_train_scaled, y_train, cv=5, scoring='accuracy')
print(f"Cross-validation scores / Pontua√ß√µes de valida√ß√£o cruzada: {cv_scores}")
print(f"Mean CV accuracy / Acur√°cia m√©dia CV: {cv_scores.mean():.4f} (+/- {cv_scores.std() * 2:.4f})")

# Example of hyperparameter tuning / Exemplo de ajuste de hiperpar√¢metros
from sklearn.model_selection import GridSearchCV

param_grid = {
    'n_estimators': [50, 100, 200],
    'max_depth': [10, 20, None],
    'min_samples_split': [2, 5, 10]
}

grid_search = GridSearchCV(RandomForestClassifier(random_state=42), 
                          param_grid, cv=3, scoring='accuracy', n_jobs=-1)
grid_search.fit(X_train_scaled, y_train)

print(f"Best parameters / Melhores par√¢metros: {grid_search.best_params_}")
print(f"Best cross-validation score / Melhor pontua√ß√£o de valida√ß√£o cruzada: {grid_search.best_score_:.4f}")
```

## Next Steps / Pr√≥ximos Passos

### English
1. Explore more advanced algorithms (XGBoost, LightGBM, Neural Networks)
2. Learn about feature selection techniques
3. Understand different evaluation metrics for your specific problem
4. Practice with different types of datasets (time series, text, images)
5. Learn about model deployment and monitoring

### Portugu√™s
1. Explore algoritmos mais avan√ßados (XGBoost, LightGBM, Redes Neurais)
2. Aprenda sobre t√©cnicas de sele√ß√£o de caracter√≠sticas
3. Entenda diferentes m√©tricas de avalia√ß√£o para seu problema espec√≠fico
4. Pratique com diferentes tipos de conjuntos de dados (s√©ries temporais, texto, imagens)
5. Aprenda sobre implanta√ß√£o e monitoramento de modelos

## Resources / Recursos

### English
- [Scikit-learn Documentation](https://scikit-learn.org/stable/)
- [Pandas Documentation](https://pandas.pydata.org/docs/)
- [Matplotlib Gallery](https://matplotlib.org/stable/gallery/index.html)
- [Seaborn Tutorial](https://seaborn.pydata.org/tutorial.html)

### Portugu√™s
- [Documenta√ß√£o Scikit-learn](https://scikit-learn.org/stable/)
- [Documenta√ß√£o Pandas](https://pandas.pydata.org/docs/)
- [Galeria Matplotlib](https://matplotlib.org/stable/gallery/index.html)
- [Tutorial Seaborn](https://seaborn.pydata.org/tutorial.html)

---

*Happy Data Science! / Feliz Ci√™ncia de Dados!* üöÄüìäü§ñ
